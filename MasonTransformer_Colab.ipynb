{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mason Transformer — Colab Training\n",
    "**125M-parameter GPT-style model for construction AI**\n",
    "\n",
    "### Before running:\n",
    "1. Go to **Runtime → Change runtime type → T4 GPU → Save**\n",
    "2. Then run cells top to bottom with Shift+Enter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 1: Clone repo and install ─────────────────────────────────────────\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/contech1'):\n",
    "    !git clone https://github.com/masonearl/contech1.git /content/contech1\n",
    "else:\n",
    "    print('Already cloned — pulling latest...')\n",
    "    !git -C /content/contech1 pull\n",
    "\n",
    "os.chdir('/content/contech1')\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "!pip install -q torch openpyxl\n",
    "\n",
    "# Verify all required files are present\n",
    "required = ['build_corpus.py','config.py','tokenizer.py','data.py',\n",
    "            'generate.py','model.py','train.py',\n",
    "            'data/materials.json','data/labor.json','data/equipment.json',\n",
    "            'data/production_rates.json','data/terms.json']\n",
    "missing = [f for f in required if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print(f'MISSING FILES: {missing}')\n",
    "else:\n",
    "    print('All required files present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 2: Check GPU ───────────────────────────────────────────────────────\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('NO GPU DETECTED.')\n",
    "    print('Go to Runtime → Change runtime type → T4 GPU, then restart.')\n",
    "else:\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU:      {name}')\n",
    "    print(f'VRAM:     {mem:.1f} GB')\n",
    "    print(f'CUDA:     {torch.version.cuda}')\n",
    "    print(f'PyTorch:  {torch.__version__}')\n",
    "    print(f'bfloat16: {torch.cuda.is_bf16_supported()}')\n",
    "    print()\n",
    "    if mem >= 35:\n",
    "        print('A100 detected — will use batch_size=8')\n",
    "    else:\n",
    "        print('T4 detected — will use batch_size=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 3: Build training corpus ──────────────────────────────────────────\n",
    "# Builds all training text from JSON rate files, conversations,\n",
    "# construction industry knowledge, and real Tempest project data.\n",
    "# Takes ~1-2 minutes.\n",
    "\n",
    "!python build_corpus.py\n",
    "\n",
    "import os\n",
    "if os.path.exists('corpus/real_projects.txt'):\n",
    "    kb = os.path.getsize('corpus/real_projects.txt') / 1024\n",
    "    print(f'Real project data loaded ({kb:.0f} KB)')\n",
    "\n",
    "if os.path.exists('corpus/full_corpus.txt'):\n",
    "    mb = os.path.getsize('corpus/full_corpus.txt') / 1e6\n",
    "    print(f'Full corpus: {mb:.1f} MB')\n",
    "else:\n",
    "    print('ERROR: full_corpus.txt not created — check errors above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: Train tokenizer ─────────────────────────────────────────────────\n",
    "# BPE tokenizer trained on the corpus. Takes ~30 seconds.\n",
    "\n",
    "!python tokenizer.py --train\n",
    "\n",
    "import os\n",
    "if os.path.exists('tokenizer.json'):\n",
    "    kb = os.path.getsize('tokenizer.json') / 1024\n",
    "    print(f'Tokenizer saved ({kb:.0f} KB)')\n",
    "else:\n",
    "    print('ERROR: tokenizer.json not created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: Train the model ─────────────────────────────────────────────────\n",
    "# This is the long cell. Leave it running — do NOT close the tab.\n",
    "#\n",
    "# What to expect:\n",
    "#   step    50/40000 | loss 9.xx | lr 1.2e-04 | 3,000 tok/s\n",
    "#   step   500/40000 | loss 4.xx | ...\n",
    "#   step  2000/40000 | loss 2.xx | ...\n",
    "#\n",
    "# Loss MUST drop below 3.0 by step 2000 or something is wrong.\n",
    "# If you see OOM (out of memory), change BATCH_SIZE = 2 below.\n",
    "#\n",
    "# T4 GPU:  ~3-5 hours for 40k steps\n",
    "# A100:    ~45-90 minutes for 40k steps\n",
    "\n",
    "import torch\n",
    "\n",
    "STEPS      = 40000\n",
    "BATCH_SIZE = 4      # Change to 2 if you get OOM errors\n",
    "RESUME     = False  # Change to True to resume from a previous checkpoint\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if mem >= 35:  # A100\n",
    "        BATCH_SIZE = 8\n",
    "        print(f'A100 ({mem:.0f} GB) — using batch_size={BATCH_SIZE}')\n",
    "    else:\n",
    "        print(f'T4 ({mem:.0f} GB) — using batch_size={BATCH_SIZE}')\n",
    "\n",
    "cmd = f'python train.py --steps {STEPS} --batch-size {BATCH_SIZE}'\n",
    "if RESUME:\n",
    "    cmd += ' --resume'\n",
    "\n",
    "print(f'Command: {cmd}')\n",
    "print('=' * 60)\n",
    "!PYTHONUNBUFFERED=1 {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: Download the trained model ─────────────────────────────────────\n",
    "# Run this AFTER Cell 5 finishes.\n",
    "# Downloads best.pt and tokenizer.json to your Mac.\n",
    "#\n",
    "# Then on your Mac, move them to:\n",
    "#   pages/contech/estimator/model/transformer/checkpoints/\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "downloaded = []\n",
    "\n",
    "for path in ['checkpoints/best.pt', 'checkpoints/latest.pt']:\n",
    "    if os.path.exists(path):\n",
    "        mb = os.path.getsize(path) / 1e6\n",
    "        print(f'Downloading {path} ({mb:.0f} MB)...')\n",
    "        files.download(path)\n",
    "        downloaded.append(path)\n",
    "        break\n",
    "\n",
    "if os.path.exists('tokenizer.json'):\n",
    "    files.download('tokenizer.json')\n",
    "    downloaded.append('tokenizer.json')\n",
    "\n",
    "if downloaded:\n",
    "    print(f'Downloaded: {downloaded}')\n",
    "    print()\n",
    "    print('Next steps:')\n",
    "    print('  1. Move both files to: pages/contech/estimator/model/transformer/checkpoints/')\n",
    "    print('  2. Tell Mason to deploy them to the live site')\n",
    "else:\n",
    "    print('No checkpoints found. Make sure Cell 5 completed successfully.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
