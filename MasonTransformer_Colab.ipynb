{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mason Transformer — Colab Training\n",
    "**125M-parameter GPT-style model for construction AI**\n",
    "\n",
    "### Before you run:\n",
    "1. **Runtime → Change runtime type → T4 GPU → Save**\n",
    "2. Then **Runtime → Run all** (or run cells one by one with Shift+Enter)\n",
    "\n",
    "Cells 1–4 take ~5 minutes. Cell 5 runs for 3–5 hours on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 1 — Clone repo and install dependencies\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "import os, subprocess\n",
    "\n",
    "REPO = '/content/contech1'\n",
    "\n",
    "if not os.path.exists(REPO):\n",
    "    print('Cloning contech1...')\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/masonearl/contech1.git', REPO], check=True)\n",
    "else:\n",
    "    print('Repo exists — pulling latest fixes...')\n",
    "    subprocess.run(['git', '-C', REPO, 'pull'], check=True)\n",
    "\n",
    "os.chdir(REPO)\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "subprocess.run(['pip', 'install', '-q', 'torch', 'openpyxl'], check=True)\n",
    "\n",
    "# Verify all required files are present\n",
    "required = [\n",
    "    'build_corpus.py', 'config.py', 'tokenizer.py', 'data.py',\n",
    "    'generate.py', 'model.py', 'train.py',\n",
    "    'data/materials.json', 'data/labor.json', 'data/equipment.json',\n",
    "    'data/production_rates.json', 'data/terms.json',\n",
    "]\n",
    "missing = [f for f in required if not os.path.exists(f)]\n",
    "if missing:\n",
    "    raise RuntimeError(f'MISSING FILES: {missing}')\n",
    "print('\\nAll required files present. Ready to train.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 2 — Check GPU\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        'NO GPU DETECTED.\\n'\n",
    "        'Go to Runtime → Change runtime type → T4 GPU, then restart.'\n",
    "    )\n",
    "\n",
    "name = torch.cuda.get_device_name(0)\n",
    "mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'GPU:      {name}')\n",
    "print(f'VRAM:     {mem:.1f} GB')\n",
    "print(f'CUDA:     {torch.version.cuda}')\n",
    "print(f'PyTorch:  {torch.__version__}')\n",
    "print(f'bfloat16: {torch.cuda.is_bf16_supported()}')\n",
    "print()\n",
    "if mem >= 35:\n",
    "    print('A100 detected — will use batch_size=8  (~45-90 min)')\n",
    "else:\n",
    "    print('T4 detected  — will use batch_size=4  (~3-5 hours)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 3 — Build training corpus\n",
    "# Combines JSON rate files, construction knowledge, real Tempest\n",
    "# project data, and synthetic conversations into one corpus.\n",
    "# Takes ~1-2 minutes.\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "import subprocess, os\n",
    "\n",
    "result = subprocess.run(['python', 'build_corpus.py'], capture_output=False)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError('build_corpus.py failed — check output above')\n",
    "\n",
    "if not os.path.exists('corpus/full_corpus.txt'):\n",
    "    raise RuntimeError('corpus/full_corpus.txt not found — build_corpus.py did not complete')\n",
    "\n",
    "mb = os.path.getsize('corpus/full_corpus.txt') / 1e6\n",
    "print(f'\\nCorpus built: {mb:.1f} MB')\n",
    "\n",
    "if os.path.exists('corpus/real_projects.txt'):\n",
    "    kb = os.path.getsize('corpus/real_projects.txt') / 1024\n",
    "    print(f'Real Tempest project data: {kb:.0f} KB included')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 4 — Train tokenizer\n",
    "# BPE tokenizer trained on the corpus. Takes ~30 seconds.\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "import subprocess, os\n",
    "\n",
    "result = subprocess.run(['python', 'tokenizer.py', '--train'], capture_output=False)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError('tokenizer.py failed — check output above')\n",
    "\n",
    "if not os.path.exists('tokenizer.json'):\n",
    "    raise RuntimeError('tokenizer.json not created')\n",
    "\n",
    "kb = os.path.getsize('tokenizer.json') / 1024\n",
    "print(f'\\nTokenizer saved: {kb:.0f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 5 — Train the model\n",
    "#\n",
    "# LEAVE THIS RUNNING — do NOT close the tab.\n",
    "# Checkpoints save every 500 steps so you won't lose progress.\n",
    "#\n",
    "# What you should see:\n",
    "#   step    50/40000 | loss 9.xx   <-- starting high, normal\n",
    "#   step   500/40000 | loss 4-5    <-- dropping, good\n",
    "#   step  2000/40000 | loss 2-3    <-- below 3.0 = working\n",
    "#\n",
    "# If loss is still >7 at step 500, something is wrong.\n",
    "# If you get OOM, set BATCH_SIZE = 2 below and re-run this cell.\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "import torch, os\n",
    "\n",
    "STEPS      = 40000\n",
    "BATCH_SIZE = 4       # Change to 2 if OOM, 8 on A100\n",
    "RESUME     = False   # Change to True to resume from checkpoint\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if mem >= 35:\n",
    "        BATCH_SIZE = 8\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} — batch_size={BATCH_SIZE}')\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "cmd = f'python train.py --steps {STEPS} --batch-size {BATCH_SIZE}'\n",
    "if RESUME:\n",
    "    cmd += ' --resume'\n",
    "\n",
    "print(f'Running: {cmd}')\n",
    "print('=' * 60)\n",
    "!PYTHONUNBUFFERED=1 {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 6 — Download the trained model\n",
    "#\n",
    "# Run this AFTER Cell 5 finishes.\n",
    "# Downloads best.pt and tokenizer.json to your Mac.\n",
    "#\n",
    "# Then on your Mac, move both files to:\n",
    "#   pages/contech/estimator/model/transformer/checkpoints/\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print('Looking for checkpoint...')\n",
    "downloaded = []\n",
    "\n",
    "for path in ['checkpoints/best.pt', 'checkpoints/latest.pt']:\n",
    "    if os.path.exists(path):\n",
    "        mb = os.path.getsize(path) / 1e6\n",
    "        print(f'Downloading {path} ({mb:.0f} MB)...')\n",
    "        files.download(path)\n",
    "        downloaded.append(path)\n",
    "        break\n",
    "\n",
    "if os.path.exists('tokenizer.json'):\n",
    "    print('Downloading tokenizer.json...')\n",
    "    files.download('tokenizer.json')\n",
    "    downloaded.append('tokenizer.json')\n",
    "\n",
    "if downloaded:\n",
    "    print(f'\\nDownloaded: {downloaded}')\n",
    "    print('\\nNext steps on your Mac:')\n",
    "    print('  Move both files to:')\n",
    "    print('  pages/contech/estimator/model/transformer/checkpoints/')\n",
    "    print('  Then tell Mason to deploy them.')\n",
    "else:\n",
    "    print('No checkpoints found.')\n",
    "    print('Make sure Cell 5 ran to completion before running this.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
