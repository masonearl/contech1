{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mason Transformer - GPU Training on Google Colab\n",
        "\n",
        "This notebook trains the 125M-parameter Mason transformer on a Colab GPU.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
        "2. Sign in with your Google account\n",
        "3. Upload this notebook (File > Upload notebook)\n",
        "4. Select a GPU runtime: Runtime > Change runtime type > T4 GPU (free tier)\n",
        "5. Run all cells in order\n",
        "\n",
        "Training takes approximately 2-4 hours on a T4 GPU for 20,000 steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone Repository and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# Clone the repo using anonymous HTTPS (public repo, no auth needed)\n",
        "if not os.path.exists('masonearl.com'):\n",
        "    !GIT_TERMINAL_PROMPT=0 git clone https://github.com/masonearl/masonearl.com.git\n",
        "else:\n",
        "    print('Repo already cloned, pulling latest...')\n",
        "    !GIT_TERMINAL_PROMPT=0 git -C masonearl.com pull\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch openpyxl\n",
        "\n",
        "print('Done.')\n",
        "!ls masonearl.com/pages/contech/estimator/model/transformer/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. GPU Detection and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
        "    print(f'CUDA version: {torch.version.cuda}')\n",
        "    print(f'PyTorch version: {torch.__version__}')\n",
        "    print(f'bfloat16 supported: {torch.cuda.is_bf16_supported()}')\n",
        "else:\n",
        "    print('WARNING: No GPU detected!')\n",
        "    print('Go to Runtime > Change runtime type > T4 GPU')\n",
        "    print('Then restart this notebook.')\n",
        "\n",
        "# Set up paths\n",
        "TRANSFORMER_DIR = 'masonearl.com/pages/contech/estimator/model/transformer'\n",
        "sys.path.insert(0, TRANSFORMER_DIR)\n",
        "os.chdir(TRANSFORMER_DIR)\n",
        "print(f'\\nWorking directory: {os.getcwd()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Training Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python build_corpus.py\n",
        "\n",
        "# Note: real_projects.txt (from extract_real_projects.py) is pre-generated locally\n",
        "# and committed to the repo. It requires OneDrive access to regenerate.\n",
        "# Check if it exists:\n",
        "import os\n",
        "if os.path.exists('corpus/real_projects.txt'):\n",
        "    size = os.path.getsize('corpus/real_projects.txt') / 1024\n",
        "    print(f'real_projects.txt found ({size:.0f} KB) - real Tempest project data loaded')\n",
        "else:\n",
        "    print('WARNING: corpus/real_projects.txt not found - real project data will not be included')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python tokenizer.py --train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train the Model\n",
        "\n",
        "Speed improvements applied:\n",
        "- `torch.compile()` (PyTorch 2.0+): ~1.5-2x faster on CUDA\n",
        "- TF32 enabled on Ampere GPUs (A100): free 1.5x speedup\n",
        "- `num_workers=2` + `pin_memory=True` + `prefetch_factor=4`: faster data loading\n",
        "- Mixed precision (bfloat16 on A100, float16 on T4)\n",
        "\n",
        "Expected throughput by GPU:\n",
        "- T4 (free): ~3,000-5,000 tok/s → 40k steps ≈ 3-5 hours\n",
        "- A100 (Colab Pro+): ~15,000-25,000 tok/s → 40k steps ≈ 45-90 min\n",
        "\n",
        "Loss target: below 3.0 within 2k steps, ideally below 2.5 by 40k steps.\n",
        "If you get OOM errors, reduce `BATCH_SIZE` to 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "# Training configuration\n",
        "STEPS = 40000        # Total training steps (was 20k - more steps = better quality)\n",
        "BATCH_SIZE = 4       # Reduce to 2 if you get OOM; increase to 8 on A100\n",
        "RESUME = False       # Set to True to resume from a checkpoint\n",
        "\n",
        "# Auto-increase batch size on A100 (40GB VRAM)\n",
        "if torch.cuda.is_available():\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    if vram_gb >= 35:  # A100\n",
        "        BATCH_SIZE = 8\n",
        "        print(f'A100 detected ({vram_gb:.0f} GB) - using batch_size={BATCH_SIZE}')\n",
        "    else:\n",
        "        print(f'GPU: {vram_gb:.0f} GB - using batch_size={BATCH_SIZE}')\n",
        "\n",
        "cmd = f'python train.py --steps {STEPS} --batch-size {BATCH_SIZE}'\n",
        "if RESUME:\n",
        "    cmd += ' --resume'\n",
        "\n",
        "print(f'Running: {cmd}')\n",
        "print('=' * 60)\n",
        "!PYTHONUNBUFFERED=1 {cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from generate import MasonEngine\n",
        "\n",
        "engine = MasonEngine()\n",
        "\n",
        "# Test prompts including real Tempest project knowledge\n",
        "test_prompts = [\n",
        "    'Can you help me bid a job?',\n",
        "    'What is the T&M rate for a foreman?',\n",
        "    'How much does 8-inch PVC C900 pipe cost per foot?',\n",
        "    'What was the Herriman Old Town Waterline project?',\n",
        "    'What does a Cat 320 excavator cost per hour?',\n",
        "    'What is a typical price per foot for a 6-inch IHP gas line relocation?',\n",
        "    'How much does 3/4 inch APWA road base cost from SPC?',\n",
        "    'Estimate 1000 LF of 8-inch sewer in clay at 6 feet deep.',\n",
        "    'What production rate should I use for a Cat 315 excavator?',\n",
        "    'Hello, what can you do?',\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f'\\nUser: {prompt}')\n",
        "    response = engine.chat([\n",
        "        {'role': 'system', 'content': 'You are Mason, a personal AI assistant built by Mason Earl.'},\n",
        "        {'role': 'user', 'content': prompt}\n",
        "    ])\n",
        "    print(f'Mason: {response}')\n",
        "    print('-' * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Download Trained Model\n",
        "\n",
        "Download the best checkpoint to your local machine. Then copy it to:\n",
        "`pages/contech/estimator/model/transformer/checkpoints/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "best_path = 'checkpoints/best.pt'\n",
        "latest_path = 'checkpoints/latest.pt'\n",
        "\n",
        "if os.path.exists(best_path):\n",
        "    size_mb = os.path.getsize(best_path) / 1e6\n",
        "    print(f'Downloading best.pt ({size_mb:.0f} MB)...')\n",
        "    files.download(best_path)\n",
        "elif os.path.exists(latest_path):\n",
        "    size_mb = os.path.getsize(latest_path) / 1e6\n",
        "    print(f'Downloading latest.pt ({size_mb:.0f} MB)...')\n",
        "    files.download(latest_path)\n",
        "else:\n",
        "    print('No checkpoint found. Run the training cell first.')\n",
        "\n",
        "# Also download the tokenizer\n",
        "if os.path.exists('tokenizer.json'):\n",
        "    files.download('tokenizer.json')\n",
        "    print('Downloaded tokenizer.json')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}